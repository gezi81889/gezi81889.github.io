<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://gezi81889.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gezi81889.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-08T04:53:52+00:00</updated><id>https://gezi81889.github.io/feed.xml</id><title type="html">Gezi’s TechBlog</title><subtitle>A personal technlog :). </subtitle><entry><title type="html">Covariance and correlation</title><link href="https://gezi81889.github.io/blog/2022/Cov-and-Corr/" rel="alternate" type="text/html" title="Covariance and correlation"/><published>2022-08-12T15:12:00+00:00</published><updated>2022-08-12T15:12:00+00:00</updated><id>https://gezi81889.github.io/blog/2022/Cov-and-Corr</id><content type="html" xml:base="https://gezi81889.github.io/blog/2022/Cov-and-Corr/"><![CDATA[<h3 id="variance-vs-covariance">Variance vs. Covariance</h3> <p>Variance refers to the spread of a variable around its mean value, while a covariance refers to the measure of the directional relationship between two random variables.</p> <p>Suppose we have a dataset with variable \(X\) and \(Y\). For a variable \(X\), it has n values \([x_1,x_2...x_n]\) ,</p> \[Var(X)= \frac{1}{n-1}\sum(x_i-\overline x)^2\] <p>For another variable \(Y\) in this dataset, we can also calculate its variance. But \(Var(X)\) and \(Var(Y)\) don’t give us information about the distribution of the data in 2D space, meaning the trend of data in higher space. The covariance quantifies the <strong>direction of the trend</strong>, but it does not say how strong the trend is and covariance is very <strong>sensitive to scaling</strong>.</p> \[Cov(X,Y) = \frac{1}{n-1}\sum(x_i-\overline x)(y_i-\overline y)=E[(X-E(X))(Y-E(Y))]=E(XY)-E(X)E(Y)\] <p>For a positive covariance, \(X\) and \(Y\) are incrasing together; For a negative value, \(X\) is increasing but \(Y\) is decreasing.</p> <h3 id="correlation">Correlation</h3> <p>Correlation between two variables measures the <strong>strength of the trend</strong>. After normalization, correlation coefficient does not get affected by scaling. Both covariance and correlation are used for quantifying the “Independence” between ramdon variables.</p> \[Cor(X,Y) = \frac{Cor(X,Y)}{\sqrt{Var(X)Var(Y)}}\] <h3 id="covariance-matrix">Covariance matrix</h3> <p>Covariance matrix contain variances and covariance between variables in our data, which means we have variables in both column and row. Suppose we have \(p\) cells \(\times\) \(n\) genes in dataset \(X\) and \(q\) cells \(\times\) \(m\) genes in dataset \(Y\) (Note that \(X\) and \(Y\) are now datasets not variables ):</p> \[X=\begin {pmatrix} x_{11} &amp; x_{12} &amp; ... &amp; x_{1n} \\ x_{21} &amp; x_{22} &amp; ... &amp; x_{2n} \\ . &amp; . &amp; ... &amp; . \\ . &amp; . &amp; ... &amp; . \\ x_{p1} &amp; x_{p2} &amp; ... &amp; x_{pn} \end{pmatrix} = [\vec x_1,\vec x_2,...\vec x_p]’\] \[Y=\begin {pmatrix} y_{11} &amp; y_{12} &amp; ... &amp; y_{1n} \\ y_{21} &amp; y_{22} &amp; ... &amp; y_{2n} \\ . &amp; . &amp; ... &amp; . \\ . &amp; . &amp; ... &amp; . \\ y_{q1} &amp; y_{q2} &amp; ... &amp; y_{qn} \end{pmatrix} = [\vec y_1,\vec y_2,...\vec y_q]’\] <p>Mean-centered (each column has zero expectation):</p> \[\begin{align*}\label{1} \overline x &amp;=\frac{1}{p}\sum_{i=1}^p \vec x_{i} = [E(gene_1),E(gene_2),...E(gene_n)] \\ \widetilde X &amp;=[\vec x_1-\overline x,\vec x_2-\overline x,...\vec x_p-\overline x]'=[gene_1-E(gene_1),gene_2-E(gene_2),....gene_n-E(gene_n)] \\ \overline y &amp;=\frac{1}{q}\sum_{i=1}^q \vec y_{i}= [E(gene_1),E(gene_2),...E(gene_m)] \\ \widetilde Y &amp;=[\vec y_1-\overline y,\vec y_2-\overline y,...\vec y_q-\overline y]'=[gene_1-E(gene_1),gene_2-E(gene_2),....gene_m-E(gene_m)] \end{align*}\] <p>Covariance matrix is the matrix whose $(i,j)$ entry is the covariance between the variables $gene_i, gene_j$:</p> \[\begin{align*}\label{2} Var(\vec X)_{i,j} &amp;= Cov(gene_i, gene_j)=&gt;Var(\vec X)= Cov(\vec X,\vec X)=\frac{1}{p-1}\widetilde X^T\widetilde X \\ Var(\vec Y)_{i,j} &amp;=Cov(gene_i, gene_j)=&gt;Var(\vec Y)=Cov(\vec Y,\vec Y) = \frac{1}{q-1}\widetilde Y^T\widetilde Y \end{align*}\] <p>If we denote the variable vector as \(\vec X=[gene_1,gene_2,...gene_n]'\) (sometime also called random vectors), the covariance matrix of dataset \(X\) can also be considered as covariance matrix of the random vector \(\vec X\) , which is typically denoted by \(K_{XX}\) or \(\Sigma_{XX}\) .</p> <p><strong><em>Characteristics of Covariance matrix \(A=Var(\vec X)\):</em></strong></p> <ol> <li> <p>\(n \times n\) Square symmetric -&gt; \(A = A^T\)</p> </li> <li> <p>\(Var(\vec X)_{ii}=Cov(gene_i,gene_i)=Var(gene_i)\).</p> </li> <li> <p>Positive semi-definite -&gt; For every non-zero vector \(\vec z\), \(z'Az≥0\)</p> </li> <li> <p>If \(gene_1,gene_2,....gene_n\) are all independent,\(Var(\vec X)=diag(Var(gene_1),Var(gene_2),...Var(gene_n))\)</p> </li> </ol> <p>Similarly we can also have \(K_{XY}\) or \(\Sigma_{XY}\) , which is the <strong>cross-covariance matrix</strong> of random vector \(\vec X\) and \(\vec Y\) of dataset \(X\) and \(Y\). \((i,j)\) entry is the covariance between the variables \(gene_i\) from dataset \(X\) and \(gene_j\) from dataset \(Y\) (note that for calculation, we have to subset two datasets to make them have same observations/cells \(r\)):</p> \[Cov(\vec X,\vec Y)_{i,j} =Cov(gene_i, gene_j)=&gt;Cov(\vec X,\vec Y)= \frac{1}{r-1}\widetilde X^T\widetilde Y\] <p>Cross-covariance can measure how dependent two variable set of two datasets are, but it also depends on the unit of \(\vec X\) and \(\vec Y\), that is to say, they are also sensitive to scaling.</p> <p><strong><em>Characteristics of Cross-coariance matrix \(A=Cov(\vec X, \vec Y), B=Cov(\vec Y,\vec X)^T\):</em></strong></p> <ol> <li>\(n \times m\) matrix, in general not symmetric</li> <li>\(A= B^T\).</li> <li>\(Cov(\vec X_1+\vec X_2,\vec Y)=Cov(\vec X_1,\vec Y)+Cov(\vec X_2,\vec Y)\).</li> <li>If \(a\) is a constant, \(Cov(a\vec X,\vec Y)=Cov(\vec X,a\vec Y)=aA\) ; \(Cov(a+\vec X,\vec Y)=Cov(\vec X,a +\vec Y)=A\) (Linear recombination is discussed below)</li> <li>If \(n=m\), \(Var(\vec X+\vec Y)=Var(\vec X)+Var(\vec Y)+A+B\)</li> </ol> <h3 id="covariance-of-linear-recombinations">Covariance of linear recombinations</h3> <p>If constants \(a_i,b_i \in R\) , \(i=1,2,...,n\) , then the following is true:</p> \[\begin{align*}\label{3} &amp; Cov(a_1gene_1+a_2gene_2+....+a_ngene_n,b_1gene1+b_2gene2+...+b_ngene_n) \\ &amp; =Cov(\vec a \cdot \vec X, \vec b \cdot \vec X) = \sum_{i=1}^n\sum_{j=1}^n a_ib_jCov(gene_i,gene_j)=\vec a^T\Sigma_{XX}\vec b \end{align*}\] <p>\(\vec a=[a_1,a_2,...a_n]', \vec b=[b_1,b_2,...b_n]'\). Similarly,</p> \[\begin{align*}\label{4} &amp; Cov(a_1gene_1+a_2gene_2+....+a_ngene_n,a_1gene1+a_2gene2+...+a_ngene_n) \\ &amp; =Cov(\vec a \cdot \vec X, \vec a \cdot \vec X) = Var(\vec a \cdot \vec X)= \sum_{i=1}^n\sum_{j=1}^n a_ia_jCov(gene_i,gene_j)=\vec a^TVar(\vec X)\vec a \end{align*}\] <p>Recalled \(Var(aX)=a^2Var(X)\), \(X\) is a random variable.</p> <p>For a square \(n \times n\) matrix \(A=[\vec a_1',\vec a_2',...\vec a_n']'\) and a square \(n \times n\) matrix \(B=[\vec b_1',\vec b_2',...\vec b_n']'\), \(A\vec X\) is the vector of linear recombination of \(X_i\) variables, we have:</p> \[Var(A\vec X)= AVar(\vec X)A^T \\ Cor(A\vec X,B\vec X) = AVar(\vec X)B^T\] <p>In summary, for ramdon vectors \(\vec X\) and \(\vec Y\), \(\vec X=[X_1,X_2,...X_n], \vec Y=[Y_1,Y_2,...Y_m]\) , \(X_i\) and \(Y_i\) are random variables (In our cases above, gene variable).</p> <ol> <li> <p>\(Cov(X_i,X_i)=Var(X_i)\) is a value</p> </li> <li> <p>\(Cov(\vec X,\vec X)=Var(\vec X)=K_{XX}=\Sigma_{XX}\) is covariance matrix</p> </li> <li> <p>\(Cov(X_i,Y_i)\) is a value</p> </li> <li> <p>\(Cov(\vec X,\vec Y)=K_{XY}=\Sigma_{XY}\) is covariance matrix</p> </li> <li> <p>\(Cov(\vec a^T \vec X,\vec b^T \vec X)= Cov(X^*,X^*)\) is a value. \(X^*\) is the linear recombination of variable \(X_i\) ; Similarly, \(Cov(\vec a^T \vec X,\vec b^T \vec Y)\) is a value</p> </li> </ol> <h3 id="correlation-matrix">Correlation matrix</h3> <p>Correlation matrix contain correlations between variables in our data, which means we have variables in both column and row.\((i,j)\) entry is the correlation between the variables \(gene_i, gene_j\):</p> <p>Standardized matrix:</p> \[\begin{align*}\label{5} \sigma(gene_i) &amp;=\sqrt{Var(gene_i)} \\ X_s &amp;=[\frac{gene_1-E(gene_1)}{\sigma(gene_1)},\frac{gene_2-E(gene_2)}{\sigma(gene_2)},....\frac{gene_n-E(gene_n)}{\sigma(gene_n)}] \\ Y_s &amp;=[\frac{gene_1-E(gene_1)}{\sigma(gene_1)},\frac{gene_2-E(gene_2)}{\sigma(gene_2)},....\frac{gene_n-E(gene_m)}{\sigma(gene_m)}] \end{align*}\] <p>Correlation matrix:</p> \[\begin{align*}\label{6} &amp;Corr(\vec X)_{ij}=Cor(gene_i,gene_j)-&gt;Corr(\vec X)=\frac{1}{p-1}X_s^TX_s \\ &amp;Corr(\vec Y)_{ij}=Cor(gene_i,gene_j)-&gt;Corr(\vec Y)=\frac{1}{q-1}Y_s^TY_s \end{align*}\] <p>Similarly, we also have <strong>Cross-correlation</strong>. Similarly we need to subset the dataset to make them have same observations/cells \(r\).</p> \[\begin{align*}\label{7} Corr(\vec X,\vec Y)_{ij} &amp;=Cor(gene_i,gene_j)-&gt; Corr(\vec X,\vec Y)=\frac{1}{r-1}X^T_sY_s \\ Corr(\vec Y,\vec X)_{ij} &amp;=Cor(gene_i,gene_j)-&gt; Corr(\vec Y,\vec X)=\frac{1}{r-1}Y^T_sX_s \end{align*}\] <h3 id="reference">Reference</h3> <p>This blog heavily referenced the Course of Youtuber molypath.</p> <p><a href="https://www.youtube.com/watch?v=QptKkD__k-c">https://www.youtube.com/watch?v=QptKkD__k-c</a></p>]]></content><author><name></name></author><category term="math-notes"/><category term="math"/><category term="covariance"/><category term="correlation"/><summary type="html"><![CDATA[Variance vs. Covariance]]></summary></entry><entry><title type="html">Mathematical summary of CCA</title><link href="https://gezi81889.github.io/blog/2022/cca/" rel="alternate" type="text/html" title="Mathematical summary of CCA"/><published>2022-08-10T15:12:00+00:00</published><updated>2022-08-10T15:12:00+00:00</updated><id>https://gezi81889.github.io/blog/2022/cca</id><content type="html" xml:base="https://gezi81889.github.io/blog/2022/cca/"><![CDATA[<p><strong>Canonical Correlation Analysis (CCA)</strong> is a dimension reduction method that is similar to PCA, but it simultaneously reduce the dimension of <strong>two</strong> random vectors instead of one in PCA. Instead of trying to explain overall variance, it tries to explain the association between two random vectors, that is, the correlation between two random vectors. For example, predict cell type in a new dataset \(Y\) based on the reference dataset \(X\).</p> <p>The main purpose of the canonical correlation approach is the exploration of sample correlations between two sets of quantitative variables observed on the <strong>same</strong> experimental units (e.g same cells, different genes; or same genes, different cells ). It can reduce multiple variables into entities that can be correlated with each other. It finds the two bases in which the correlation matrix between the variables is diagonal and correlations on the diagonal are maximized.</p> <h2 id="step1canonical-variate-pair">Step1:Canonical Variate Pair</h2> <p>Suppose we have \(p\) cells \(\times\) \(n\) genes in dataset \(X\) and \(q\) cells \(\times\) \(n\) genes in dataset \(Y\) (<strong>both are mean-centered already</strong>), note that row is gene, column is cell variable:</p> \[X=\begin {pmatrix} x_{11} &amp; x_{12} &amp; ... &amp; x_{1p} \\ x_{21} &amp; x_{22} &amp; ... &amp; x_{2p} \\ . &amp; . &amp; ... &amp; . \\ . &amp; . &amp; ... &amp; . \\ x_{n1} &amp; x_{n2} &amp; ... &amp; x_{np} \end{pmatrix} = [\vec x_1,\vec x_2,...\vec x_p]\] \[Y=\begin {pmatrix} y_{11} &amp; y_{12} &amp; ... &amp; y_{1q} \\ y_{21} &amp; y_{22} &amp; ... &amp; y_{2q} \\ . &amp; . &amp; ... &amp; . \\ . &amp; . &amp; ... &amp; . \\ y_{n1} &amp; y_{n2} &amp; ... &amp; y_{nq} \end{pmatrix} = [\vec y_1,\vec y_2,...\vec y_q]\] <p>Thus, the two random vectors are:</p> <p>\(\vec X =[\vec x_1,\vec x_2,...\vec x_p]'\) \(\vec Y = [\vec y_1,\vec y_2,...\vec y_q]'\)</p> <p>Suppose \(q≤p\) . The goal of CCA is to summarise \(\Sigma_{YX}\) with \(q\) numbers, which will be called <strong>canonical correlations</strong>.</p> <p>Define a set of linear combinations named \(u_i=\vec a_i \cdot \vec X\) and \(v_i=\vec b_i \cdot \vec Y\), note that \(u_i,v_i\) are now new random variable, not random vector.</p> \[A = [\vec a_1',\vec a_2',...,\vec a_p']' ; A^T=[\vec a_1,\vec a_2,...,\vec a_p]\] \[B = [\vec b_1',\vec b_2',...,\vec b_q']'; B^T = [\vec b_1,\vec b_2,...,\vec b_q]\] \[\vec U=\begin {pmatrix} u_{1} \\ u_{2} \\ . \\ . \\ u_{p} \end{pmatrix} =\begin {pmatrix} a_1'\vec X \\ a_2' \vec X \\ . \\ . \\ a_p'\vec X \end{pmatrix} =A \vec X\] \[\vec V=\begin {pmatrix} v_{1} \\ v_{2} \\ . \\ . \\ v_{q} \end{pmatrix} =\begin {pmatrix} b_1'\vec Y \\ b_2'\vec Y \\ . \\ . \\ b_q'\vec Y \end{pmatrix} =B\vec Y\] \[U=XA^T ; V=YB^T\] <p>\(U\) is \(n \times p\) matrix, the columns of which contains the linear recombination of random variables of dataset \(X\). \(V\) is \(n\times q\) matrix, the columns of which contains the linear recombination of random variables of dataset \(Y\). For \(i≤q\), \([v_i,u_i]\) is the \(i^{th}\) <strong>canonical variate pair</strong>. We hope to find linear combinations that maximize the correlations (<strong>canonical coeffecient</strong>) between the members of each canonical variate pair.</p> <h2 id="step2-maximize-canonical-correlation">Step2: Maximize Canonical Correlation</h2> <p>We have correlation of \(i^{th}\) canonical variate pair \([v_i,u_i]\):</p> \[Cor(v_i,u_i)=\frac{Cov(v_i,u_i)}{\sqrt{Var(v_i)Var(u_i)}}= \frac{b_i^TY^TXa_i}{\sqrt{(b_i^TY^TYb_i)(a_i^TX^TXa_i)}}\] <p>Two constraints:</p> <ol> <li> <p>\(u_i,v_i\) has unit length: \(b_i^TY^TYb_i=a_i^TX^TXa_i=1\)</p> </li> <li> <p>Orthogonality: if we want to find second pair of canonical variables, these must be orthogonal or uncorrelated with the first pair. For \(i≠j,i,j≤q:\)</p> </li> </ol> \[u^T_iu_j=0 ; v^T_iv_j=0\] <p>With this notation, the goal of CCA is to find $q$ linear projections \((a_i,b_i)\) for \(i \in [ 1,2,...q]\), that maximize canonical correlation of each canonical variate pair.</p> <h2 id="step3-solve-cononical-correlation-vector">Step3: Solve Cononical Correlation Vector</h2> <p>We can use SVD to solve this problem.</p> <p>To maximize \(b_i^TY^TXa_i\), with constraints \(b^T_iY^TYb_i=1,a^T_iX^TXa_i=1\),</p> \[a_i=(X^TX)^{-\frac{1}{2}}\hat a_i -&gt; \hat a_i^T\hat a_i=1\] \[b_i=(Y^TY)^{-\frac{1}{2}}\hat b_i -&gt;\hat b_i^T\hat b_i=1\] \[b_i^TY^TXa_i=\hat b^T_i(Y^TY)^{-\frac{1}{2}}Y^TX(X^TX)^{-\frac{1}{2}}\hat a_i\] <p>Apply SVD to \(Z=(X^TX)^{-\frac{1}{2}}X^TY(Y^TY)^{-\frac{1}{2}}\):</p> \[Z=\hat AD\hat B'\] <p>We find that, by doing so, the constriants are satisfied. Cause \(\hat A^T\hat A=I,\hat B^T\hat B=I\).</p> <p>To make it easier,</p> \[B = (Y^TY)^{-\frac{1}{2}}\hat B\] \[A = (X^TX)^{-\frac{1}{2}}\hat A\] <p>\(B=[b_1,b_2,...b_q]\),\(A=[a_1,a_2,...a_q]\).</p> <p>We can also use Lagrange multipliers to solve this problem:</p> \[\mathcal{L} = b_i^TY^TXa_i' - \frac{p_{i,1}}{2}((a_i'X)^T(a_i'X)-1) - \frac{p_{i,2}}{2}((b_i'Y)^T(b_i'Y)-1)\] <p>Taking the derivatives of the loss with respect to \(b_i\):</p> \[\begin{align*}\label{1} &amp;\frac{\partial (b_i'Y^TXa_i)}{\partial \ b_i} = Y^TXa_i \\ &amp;\frac{\partial \ (\frac{p_{i,1}}{2}((a_i'X)(a_i'X)^T-1))}{\partial \ b_i} = 0 \\ &amp;\frac{\partial \ (\frac{p_{i,2}}{2}((b_i'Y)(b_i'Y)^T-1)}{\partial \ b_i} = p_{i,2}Y^TYb_i \end{align*}\] <p>The derivatives with respect to \(a_i\) are symmetric. Putting it all together:</p> \[\begin{align*}\label{2} &amp;\frac{\partial \mathcal L}{\partial a_i} = X^TYb_i - p_{i,1}X^TXa_i = 0 \\ &amp;\frac{\partial \mathcal L}{\partial b_i} = Y^TXa_i - p_{i,2}Y^TYb_i = 0 \end{align*}\] <table> <tbody> <tr> <td>Prove \(p_{i,1}=p_{i,2}\) , knowing $$</td> <td> </td> <td>v_i</td> <td> </td> <td>_2^2=1 ,</td> <td> </td> <td>u_i</td> <td> </td> <td>_2^2=1$$:</td> </tr> </tbody> </table> \[\begin{align*}\label{3} &amp;a_i^T(X^TYb_i - p_{i,1}X^TXa_i) = a_i^TX^TYb_i-p_{i,1}a_i^TX^TXa_i = a_i^TX^TYb_i-p_{i,1} = 0 \\ &amp;b_i^T(Y^TXa_i - p_{i,2}Y^TYb_i)=b_i^TY^TXa_i-p_{i,2}b_i^TY^TYb_i=b_i^TY^TXa_i-p_{i,2} = 0 \\ &amp;=&gt; p_{i,1}=p_{i,2}=a_i^TX^TYb_i=b_i^TY^TXa_i \end{align*}\] <p>That is to say, we need to find the maximum \(p_i\).</p> <p>Solve Canonical Correlation Vector:</p> \[\begin{align*}\label{4} &amp;X^TYb_i - p_iX^TXa_i = 0 \ \ -&gt; a_i = \frac{(X^TX)^{-1}X^TYb_i}{p_i} \\ &amp;Y^TXa_i - p_{i}Y^TYb_i = 0 \ \ -&gt; Y^TX\frac{(X^TX)^{-1}X^TYb_i}{p_i} - p_iY^TYb_i = 0 \\ &amp;-&gt;p^2b_i = (Y^TY)^{-1}Y^TX(X^TX)^{-1}X^TYb_i \end{align*}\] <p>This is the standard eigenvalue problem.</p> \[((Y^TY)^{-1}Y^TX(X^TX)^{-1}X^TY-p^2I)b_i=0\] <p>After solving \(b_i\), \(a_i\) can be easily solved.</p> <p>\((a_i,b_i)\) is the correlation coefficient, \(p_i\) is the correlation between canonical variates \(v_i,u_i\).</p> <p>\(\hat A,\hat B\) columns contains \(a_i,b_i\) , we can rank the columns by the eigenvalue (correlation).</p> <h3 id="reference">Reference</h3> <ol> <li><a href="https://gregorygundersen.com/blog/2018/07/17/cca/">https://gregorygundersen.com/blog/2018/07/17/cca/</a></li> <li><a href="https://www.cnblogs.com/pinard/p/6288716.html">https://www.cnblogs.com/pinard/p/6288716.html</a></li> </ol>]]></content><author><name></name></author><category term="math-notes"/><category term="math"/><category term="cca"/><summary type="html"><![CDATA[Canonical Correlation Analysis (CCA) is a dimension reduction method that is similar to PCA, but it simultaneously reduce the dimension of two random vectors instead of one in PCA. Instead of trying to explain overall variance, it tries to explain the association between two random vectors, that is, the correlation between two random vectors. For example, predict cell type in a new dataset \(Y\) based on the reference dataset \(X\).]]></summary></entry><entry><title type="html">Mathematical summary of PCA</title><link href="https://gezi81889.github.io/blog/2022/pca/" rel="alternate" type="text/html" title="Mathematical summary of PCA"/><published>2022-06-14T15:12:00+00:00</published><updated>2022-06-14T15:12:00+00:00</updated><id>https://gezi81889.github.io/blog/2022/pca</id><content type="html" xml:base="https://gezi81889.github.io/blog/2022/pca/"><![CDATA[<p>For the purpose of dimension reduction, we apply <strong>Principal Components Analysis (PCA)</strong> to transform data to principal components and obtain the contribution of the original vairables.</p> <h2 id="step1-the-covariance-matrix">Step1: The covariance matrix</h2> <p>For a given single cell RNA-seq <strong>count matrix \(X\in R^{n \times p}\)</strong> (\(n\) is the number of cells, \(p\) is the number of genes), the random vector \(\vec X\) is:</p> \[\vec X=[\vec x_1,\vec x_2,...\vec x_p]'\] <p><strong>The centered matrix:</strong></p> \[\hat \mu _j=\frac{1}{n}\sum_{i=1}^nx_{ij}\] \[\widetilde X=[\vec x_1-\hat \mu_1,\vec x_2-\hat \mu_2,...\vec x_p-\hat \mu_p]\] <p><strong>The covariance matrix:</strong></p> \[S=Var(\vec X)=\Sigma_{XX}=\frac{1}{n-1}\widetilde X^T\widetilde X\] <h2 id="step2-eigen-decomposition">Step2: Eigen-Decomposition</h2> <p>The <strong>covariance matrix \(S_{p\times p}\)</strong> is a symmatric matrix with diagonal predictor variance and non-diagonal inter-predictor covariance.</p> <p>For a <strong>real symmatric matrix</strong>, there exists a unique set of real eigenvalues \((\lambda_1,...\lambda_p)\), and the associated eigenvectors: \((\vec u_1,...\vec u_p)\). Such that:</p> \[\begin{align*}\label{1} Su_i &amp;=\lambda_iu_i \\ u_i^Tu_j &amp;=0, i≠j \ \ (orthogonal) \\ ||u_i||^2 &amp;=1 \ \ (normalized) \end{align*}\] <p>Hence, those eigenvectors \((\vec u_1,...\vec u_p)\) form an orthonormal basis for \(S\). For \(i=1,2,...p\) , both \(\lambda_i\) and \(u_i\) can be easily calculated.</p> <p><strong>Eigenvalue Spectrum:</strong></p> \[\Lambda = \begin {pmatrix} \lambda_1 &amp; 0 &amp; ... &amp; 0 \\ 0 &amp; \lambda_2 &amp; ... &amp; 0 \\ . &amp; . &amp; ... &amp; . \\ . &amp; . &amp; ... &amp; . \\ 0 &amp; 0 &amp; ... &amp; \lambda_p \end{pmatrix}\] <p><strong>Orthogonal Eigenvector Matrix:</strong></p> \[Q=\begin {pmatrix} u_{11} &amp; u_{21} &amp; ... &amp; u_{p1} \\ u_{12} &amp; u_{22} &amp; ... &amp; u_{p2} \\ . &amp; . &amp; ... &amp; . \\ . &amp; . &amp; ... &amp; . \\ u_{1p} &amp; u_{2p} &amp; ... &amp; u_{pp} \end{pmatrix} =[\vec u_1,\vec u_2,..,\vec u_p]\] \[Q^TQ=QQ^T=I \ \ &lt;=&gt; Q^{-1}=Q^T\] <p><strong>Eigen-Decomposition</strong></p> \[S=Q\Lambda Q^T\] <p><strong>Characteristics of the gram matrix S</strong></p> <ol> <li>Eigenvalues are non-negative real numbers (positive-semidefinite).</li> </ol> \[\begin{align*}\label{2} X^TXu &amp;=\lambda u \\ u^TX^TXu &amp;=u^T\lambda u \\ (Xu)^T(Xu) =\lambda u^Tu ||Xu||^2 &amp;=\lambda ||u||^2 \ =&gt; \lambda≥0 \end{align*}\] <ol> <li>\(X^TX\) and \(XX^T\) share the same eigenvalues.</li> </ol> \[\begin{align*}\label{3} X^TXu &amp;=\lambda u \\ XX^TXu &amp;=X\lambda u \\ XX^T(Xu) &amp;=\lambda(Xu) \\ XX^T\widetilde u &amp;=\lambda \widetilde u \end{align*}\] <ol> <li>The sum of eigenvalues (total sample variance) is equal to its trace.</li> </ol> \[Tr(X^TX)=Tr(U\Lambda U^T)=Tr(U^TU\Lambda)=Tr(\Lambda)=\sum_{i=1}^p\lambda_i\] <p><strong>SVD</strong></p> <p>We can also get the same conclusion from SVD:</p> \[\widetilde X=U\Sigma V^T\] <p>\(U_{n\times n}\) and \(V_{p\times p}\) are orthogonal matrix, \(\Sigma_{n\times p}\) is diagonal matrix.</p> <p><strong>Gene covariance matrix :</strong></p> \[X^T_{p\times n}X_{n\times p}=V_{p\times p}\Sigma^2_{p\times p}V^T_{p\times p}\] <p>For computation efficiency, we can directly do svd on \(X^TX\):</p> \[\widetilde X=U\Sigma V^T=[\vec u_1,\vec u_2,..,\vec u_p] \begin {pmatrix} \lambda_1 &amp; 0 &amp; ... &amp; 0 \\ 0 &amp; \lambda_2 &amp; ... &amp; 0 \\ . &amp; . &amp; ... &amp; . \\ . &amp; . &amp; ... &amp; . \\ 0 &amp; 0 &amp; ... &amp; \lambda_p \end{pmatrix} [\vec v_1,\vec v_2,..,\vec v_p]'\] \[S=\frac{1}{n-1}X^TX=X^TX=[\vec u_1,\vec u_2,..,\vec u_p] \begin {pmatrix} \frac{\lambda_1^2}{n-1} &amp; 0 &amp; ... &amp; 0 \\ 0 &amp; \frac{\lambda_2^2}{n-1} &amp; ... &amp; 0 \\ . &amp; . &amp; ... &amp; . \\ . &amp; . &amp; ... &amp; . \\ 0 &amp; 0 &amp; ... &amp; \frac{\lambda_p^2}{n-1} \end{pmatrix} [\vec v_1,\vec v_2,..,\vec v_p]'\] <h2 id="step3-embeddings-and-loadings">Step3: Embeddings and Loadings</h2> <p>Here we first define <strong>Principle component</strong> as the new embeddings:</p> \[\vec{PC} = [PC_1,PC_2,...,PC_p]= XV = U\Sigma\] <p>If we compute the covariance matrix of random vector \(\vec {PC}\),</p> \[Var(\vec {PC})= \frac{1}{n-1}PC^TPC=\frac{1}{n-1}(XV)^TXV=\frac{1}{n-1}\Sigma^2\] <p>We can see that:</p> <ol> <li>For \(i,j \in [1,p], i≠j\), \(PC_i\) and \(PC_j\) is independent. Or we can say new embeddings are orthonormal.</li> <li>\(Var(PC_i)=\lambda_i^2\) .</li> </ol> <p>If we sort columns in \(\Sigma\) based on the value of \(\lambda_i\) , then the new PC embeddings are also sorted based on their variance. This is the premise of dimension reduction.</p> <p><strong>Loadings</strong> is the coefficient of original “coordinate” (expression level). \(\vec v_i\) is the loadings of \(PC_i\) (a specific observation/direction of data)</p> \[[EB_1,EB_2,...EB_p]=V=[\vec v_1,\vec v_2,..,\vec v_p]\] <p><em>Example:</em> For the first pricipal component <strong><em>PC1</em></strong>, the embeddings is the linear combination of the original variables. \(x_{11},x_{12},x_{13}...x_{1p}\) are the original expression value of <em>cell 1</em>.</p> \[PC_{1,cell1} =\vec x_1 \cdot \vec v_1= v_{11}x_{11}+v_{12}x_{12}+...v_{1p}x_{1p}\] <p>\(\vec v_1\) corresponds to the first eigenvector of the covariance matrix. The elements of the \(v_{1i}\) is the loading for <em>gene i</em>.</p> <p>For the purpose of dimension reduction, we will only pick $k(k&lt;p)$ PCs.</p> \[Z=XV=U\Sigma\] \[Z_{n \times k}=X_{n\times p}V_{p\times k}=U_{n\times k}\Sigma_{k\times k}\] <p>\(Z_{n\times k}\) are low dimensional embeddings, containing \(k\) PCs.</p> <h3 id="reference">Reference</h3> <p>This blog slightly referenced the Course of Youtuber Steve Brunton.</p> <p><a href="https://www.youtube.com/watch?v=fkf4IBRSeEc">https://www.youtube.com/watch?v=fkf4IBRSeEc</a></p>]]></content><author><name></name></author><category term="math-notes"/><category term="math"/><category term="pca"/><summary type="html"><![CDATA[For the purpose of dimension reduction, we apply Principal Components Analysis (PCA) to transform data to principal components and obtain the contribution of the original vairables.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://gezi81889.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://gezi81889.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://gezi81889.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry></feed>