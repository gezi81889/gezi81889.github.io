---
layout: post
title: Covariance and correlation
date: 2022-06-14 11:12:00-0400
comments: true
description: 
tags: math pca 
categories: math-notes
---

# Mathematical summary of PCA

For the purpose of dimension reduction, we apply **Principal Components Analysis (PCA)** to transform data to principal components and obtain the contribution of the original vairables. 



## Step1: The covariance matrix 

For a given single cell RNA-seq **count matrix $$X\in R^{n \times p}$$** ($$n$$ is the number of cells, $$p$$ is the number of genes), the random vector $$\vec X$$ is: 

$$
\vec X=[\vec x_1,\vec x_2,...\vec x_p]'
$$

**The centered matrix:** 

$$
\hat \mu _j=\frac{1}{n}\sum_{i=1}^nx_{ij}
$$

$$
\widetilde X=[\vec x_1-\hat \mu_1,\vec x_2-\hat \mu_2,...\vec x_p-\hat \mu_p]
$$

**The covariance matrix:** 

$$
S=Var(\vec X)=\Sigma_{XX}=\frac{1}{n-1}\widetilde X^T\widetilde X
$$

## Step2: Eigen-Decomposition

The **covariance matrix $$S_{p\times p}$$** is a symmatric matrix with diagonal predictor variance and non-diagonal inter-predictor covariance.

For a **real symmatric matrix**, there exists a unique set of real eigenvalues $$(\lambda_1,...\lambda_p)$$, and the associated eigenvectors: $$(\vec u_1,...\vec u_p)$$. Such that:

$$
\begin{align*}\label{1}
Su_i &=\lambda_iu_i
\\
u_i^Tu_j &=0, i≠j \  \  (orthogonal)
\\
||u_i||^2 &=1  \  \ (normalized)
\end{align}
$$

Hence, those eigenvectors $$(\vec u_1,...\vec u_p)$$ form an orthonormal basis for $$S$$. For $$i=1,2,...p$$ , both $$\lambda_i$$ and $$u_i$$ can be easily calculated. 

**Eigenvalue Spectrum:** 

$$
\Lambda = \begin {pmatrix} \lambda_1 & 0 & ... & 0 \\ 
                  0 & \lambda_2 & ... & 0 \\
                  . & . & ... & . \\
                  . & . & ... & . \\
                  0 & 0 & ... & \lambda_p \end{pmatrix}
$$

**Orthogonal Eigenvector Matrix:**

$$
Q=\begin {pmatrix} u_{11} & u_{21} & ... & u_{p1} \\
                   u_{12} & u_{22} & ... & u_{p2} \\
                   . & . & ... & . \\
                   . & . & ... & . \\
                   u_{1p} & u_{2p} & ... & u_{pp}  \end{pmatrix}
                   =[\vec u_1,\vec u_2,..,\vec u_p]
$$

$$
Q^TQ=QQ^T=I \ \ <=> Q^{-1}=Q^T
$$

**Eigen-Decomposition**

$$
S=Q\Lambda Q^T
$$

**Characteristics of the gram matrix S**

1. Eigenvalues are non-negative real numbers (positive-semidefinite).

$$
\begin{align*}\label{2}
X^TXu &=\lambda u
\\
u^TX^TXu &=u^T\lambda u
\\
(Xu)^T(Xu) =\lambda u^Tu
||Xu||^2 &=\lambda ||u||^2 \ => \lambda≥0
\end{align}
$$

2. $$X^TX$$ and $$XX^T$$ share the same eigenvalues. 

$$
\begin{align*}\label{3}
X^TXu &=\lambda u
\\
XX^TXu &=X\lambda u
\\
XX^T(Xu) &=\lambda(Xu)
\\
XX^T\widetilde u &=\lambda \widetilde u
\end{align}
$$

3. The sum of eigenvalues (total sample variance) is equal to its trace.
   
$$
   Tr(X^TX)=Tr(U\Lambda U^T)=Tr(U^TU\Lambda)=Tr(\Lambda)=\sum_{i=1}^p\lambda_i
$$

**SVD** 

We can also get the same conclusion from SVD: 

$$
\widetilde X=U\Sigma V^T
$$

$$U_{n\times n}$$ and $$V_{p\times p}$$ are orthogonal matrix, $$\Sigma_{n\times p}$$ is diagonal matrix. 

**Gene covariance matrix :**

$$
 X^T_{p\times n}X_{n\times p}=V_{p\times p}\Sigma^2_{p\times p}V^T_{p\times p} 
$$



For computation efficiency, we can directly do svd on $$X^TX$$:

$$
\widetilde X=U\Sigma V^T=[\vec u_1,\vec u_2,..,\vec u_p] 
     \begin {pmatrix} \lambda_1 & 0 & ... & 0 \\ 
                  0 & \lambda_2 & ... & 0 \\
                  . & . & ... & . \\
                  . & . & ... & . \\
                  0 & 0 & ... & \lambda_p \end{pmatrix}
     [\vec v_1,\vec v_2,..,\vec v_p]'
$$

$$
S=\frac{1}{n-1}X^TX=X^TX=[\vec u_1,\vec u_2,..,\vec u_p]
     \begin {pmatrix} \frac{\lambda_1^2}{n-1} & 0 & ... & 0 \\ 
                  0 & \frac{\lambda_2^2}{n-1} & ... & 0 \\
                  . & . & ... & . \\
                  . & . & ... & . \\
                  0 & 0 & ... & \frac{\lambda_p^2}{n-1} \end{pmatrix}
     [\vec v_1,\vec v_2,..,\vec v_p]'
$$



## Step3: Embeddings and Loadings

Here we first define **Principle component**  as the new embeddings: 

$$
\vec{PC} = [PC_1,PC_2,...,PC_p]= XV = U\Sigma
$$

If we compute the covariance matrix of random vector $$\vec {PC}$$, 

$$
Var(\vec {PC})= \frac{1}{n-1}PC^TPC=\frac{1}{n-1}(XV)^TXV=\frac{1}{n-1}\Sigma^2
$$

We can see that: 

1. For $$i,j \in [1,p], i≠j$$, $$PC_i$$ and $$PC_j$$ is independent. Or we can say new embeddings are orthonormal. 
2. $$Var(PC_i)=\lambda_i^2$$  . 

If we sort columns in $$\Sigma$$ based on the value of $$\lambda_i$$ , then the new PC embeddings are also sorted based on their variance.  This is the premise of dimension reduction. 

**Loadings** is the coefficient of original “coordinate” (expression level). $$\vec v_i$$ is the loadings of  $$PC_i$$ (a specific observation/direction of data)

$$
[EB_1,EB_2,...EB_p]=V=[\vec v_1,\vec v_2,..,\vec v_p]
$$

*Example:* For the first pricipal component ***PC1***, the embeddings is the linear combination of the original variables. $$x_{11},x_{12},x_{13}...x_{1p}$$ are the original expression value of *cell 1*. 

$$
PC_{1,cell1} =\vec x_1 \cdot \vec v_1=  v_{11}x_{11}+v_{12}x_{12}+...v_{1p}x_{1p}
$$

$$\vec v_1$$ corresponds to the first eigenvector of the covariance matrix. The elements of the $$v_{1i}$$ is the loading for *gene i*. 



For the purpose of dimension reduction, we will only pick $k(k<p)$ PCs. 

$$
Z=XV=U\Sigma
$$

$$
Z_{n \times k}=X_{n\times p}V_{p\times k}=U_{n\times k}\Sigma_{k\times k}
$$

$$Z_{n\times k}$$ are low dimensional embeddings, containing $$k$$ PCs. 



### Reference 

This blog slightly referenced the Course of Youtuber Steve Brunton. 

<https://www.youtube.com/watch?v=fkf4IBRSeEc>
